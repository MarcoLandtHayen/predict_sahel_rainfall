{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7dbe65-6dc0-41a5-aa96-d9a0653e54d6",
   "metadata": {},
   "source": [
    "### Predict Sahel rainfall with linear regression\n",
    "\n",
    "In this project we work with **C**limate **I**ndex **C**ollection based on **Mo**del **D**ata (CICMoD) data set (https://github.com/MarcoLandtHayen/climate_index_collection). \n",
    "\n",
    "Here, we will try to **predict future** Sahel rainfall (lead times 1 / 3 / 6 months) from current and past information (t<=0) of all input features (including PREC_SAHEL) with **CNN/fc** models:\n",
    "\n",
    "- Prepare inputs and targets.\n",
    "- Set up model.\n",
    "- Evaluate model performance.\n",
    "\n",
    "**Note:** We start with predicting future Sahel rainfall from its own history alone, hence with **univariate** inputs. Then, we add further input features to have **multivariate** inputs. And ultimately, we add **months as additional input features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca00440-1fbe-473b-be11-d7cac0887060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from json import dump, load\n",
    "from pathlib import Path\n",
    "\n",
    "### Import additional functions:\n",
    "from predict_sahel_rainfall.plot import bar_color\n",
    "from predict_sahel_rainfall.preprocessing import prepare_inputs_and_target\n",
    "from predict_sahel_rainfall.models import set_CNN_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7b72d",
   "metadata": {},
   "source": [
    "### Prepare inputs and targets: Univariate\n",
    "\n",
    "Load collection of climate indices directly from GitHub release.\n",
    "Use the complete preprocessing pipeline function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e280679",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set common parameters (except ESN and lead time) for data preprocessing:\n",
    "\n",
    "# Set url to csv file containing CICMoD indices from desired release:\n",
    "data_url = (\n",
    "    \"https://github.com/MarcoLandtHayen/climate_index_collection/\"\n",
    "    \"releases/download/v2023.03.29.1/climate_indices.csv\"\n",
    ")\n",
    "\n",
    "# Select target index:\n",
    "target_index = 'PREC_SAHEL'\n",
    "\n",
    "# Select all input features:\n",
    "input_features = [\n",
    "    'AMO', 'ENSO_12', 'ENSO_3', 'ENSO_34', 'ENSO_4', 'NAO_PC', 'NAO_ST', \n",
    "    'NP', 'PDO_PC', 'PREC_SAHEL', 'SAM_PC', 'SAM_ZM', 'SAT_N_ALL', 'SAT_N_LAND',\n",
    "    'SAT_N_OCEAN', 'SAT_S_ALL', 'SAT_S_LAND', 'SAT_S_OCEAN', 'SOI',\n",
    "    'SSS_ENA', 'SSS_NA', 'SSS_SA', 'SSS_WNA', 'SST_ESIO', 'SST_HMDR',\n",
    "    'SST_MED', 'SST_TNA', 'SST_TSA', 'SST_WSIO'\n",
    "]\n",
    "\n",
    "# # Select subset of input features:\n",
    "# input_features = [\n",
    "#     'PREC_SAHEL',\n",
    "# ]\n",
    "\n",
    "# Choose, whether to add months as one-hot encoded features:\n",
    "add_months = True\n",
    "\n",
    "# Choose, whether to normalize target index:\n",
    "norm_target = True\n",
    "\n",
    "# Specify input length:\n",
    "input_length = 24\n",
    "\n",
    "# Specify amount of combined training and validation data relative to test data:\n",
    "train_test_split = 0.9\n",
    "\n",
    "# Specify relative amount of combined training and validation used for training:\n",
    "train_val_split = 0.8\n",
    "\n",
    "## Optionally choose to scale or normalize input features according to statistics from training data:\n",
    "# 'no': Keep raw input features.\n",
    "# 'scale_01': Scale input features with min/max scaling to [0,1].\n",
    "# 'scale_11': Scale input features with min/max scaling to [-1,1].\n",
    "# 'norm': Normalize input features, hence subtract mean and divide by std dev.\n",
    "scale_norm = 'scale_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a39753e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for CNN/fc model:\n",
    "max_pooling_threshold=20\n",
    "CNN_filters = [10,20]\n",
    "CNN_kernel_sizes = [5,5]\n",
    "batch_normalization = True\n",
    "alpha = 0.3\n",
    "fc_units = [20,10]\n",
    "fc_activation = 'sigmoid'\n",
    "output_activation = 'linear'\n",
    "CNN_weight_init = 'glorot_uniform'\n",
    "CNN_bias_init = 'zeros'\n",
    "fc_weight_init = 'glorot_uniform'\n",
    "fc_bias_init = 'zeros'\n",
    "CNN_weight_reg = None\n",
    "CNN_bias_reg = None\n",
    "fc_weight_reg = None\n",
    "fc_bias_reg = None\n",
    "learning_rate = 0.0002\n",
    "loss_function = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec18f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features: 41\n"
     ]
    }
   ],
   "source": [
    "# Set choice of ESMs:\n",
    "ESMs = ['CESM', 'FOCI']\n",
    "\n",
    "# Set choice of lead times:\n",
    "lead_times = [1,3,6]\n",
    "\n",
    "# Set number of runs per setting:\n",
    "n_runs = 3\n",
    "\n",
    "# Set number of training epochs:\n",
    "n_epochs = 50\n",
    "\n",
    "# Set batch size:\n",
    "batch_size = 20\n",
    "\n",
    "# Get number of input features, depending on whether or not months are addes as additional features:\n",
    "if add_months:\n",
    "    n_features = len(input_features) + 12\n",
    "else:\n",
    "    n_features = len(input_features)\n",
    "    \n",
    "# Check number of input channels:\n",
    "print('Number of input features:',n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9b00f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM: 1 of 2\n",
      "  lead time: 1 of 3\n",
      "    run: 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 09:26:01.926861: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 09:26:01.994785: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "  lead time: 2 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "  lead time: 3 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "ESM: 2 of 2\n",
      "  lead time: 1 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "  lead time: 2 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "  lead time: 3 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n"
     ]
    }
   ],
   "source": [
    "## Initializs storages for loss curves and correlation, dimension (#ESMs, #lead times, #runs, #epochs+1).\n",
    "## Need #epochs+1, since we want to store results for untrained model plus after each epoch.\n",
    "train_loss_all = np.zeros((len(ESMs),len(lead_times),n_runs,n_epochs+1))\n",
    "val_loss_all = np.zeros((len(ESMs),len(lead_times),n_runs,n_epochs+1))\n",
    "test_loss_all = np.zeros((len(ESMs),len(lead_times),n_runs,n_epochs+1))\n",
    "train_correl_all = np.zeros((len(ESMs),len(lead_times),n_runs,n_epochs+1))\n",
    "val_correl_all = np.zeros((len(ESMs),len(lead_times),n_runs,n_epochs+1))\n",
    "test_correl_all = np.zeros((len(ESMs),len(lead_times),n_runs,n_epochs+1))\n",
    "\n",
    "## Loop over ESMs:\n",
    "for m in range(len(ESMs)):\n",
    "    \n",
    "    # Get current ESM:\n",
    "    ESM = ESMs[m]\n",
    "    \n",
    "    # Print status:\n",
    "    print('ESM:',m+1,'of',len(ESMs))\n",
    "\n",
    "    ## Loop over lead times:\n",
    "    for l in range(len(lead_times)):\n",
    "        \n",
    "        # Get current lead time:\n",
    "        lead_time = lead_times[l]\n",
    "        \n",
    "        # Print status:\n",
    "        print('  lead time:',l+1,'of',len(lead_times))\n",
    "\n",
    "        # Prepare inputs and target for current ESM and lead time:\n",
    "        (\n",
    "            train_input,\n",
    "            train_target,\n",
    "            val_input,\n",
    "            val_target,\n",
    "            test_input,\n",
    "            test_target,\n",
    "            train_mean,\n",
    "            train_std,\n",
    "            train_min,\n",
    "            train_max,\n",
    "        ) = prepare_inputs_and_target(    \n",
    "            data_url=data_url,\n",
    "            ESM=ESM,\n",
    "            target_index=target_index,\n",
    "            input_features=input_features,\n",
    "            add_months=add_months,\n",
    "            norm_target=norm_target,\n",
    "            lead_time=lead_time,\n",
    "            input_length=input_length,\n",
    "            train_test_split=train_test_split,\n",
    "            train_val_split=train_val_split,\n",
    "            scale_norm=scale_norm,\n",
    "        )\n",
    "        \n",
    "        # Loop over desired number of training runs:\n",
    "        for r in range(n_runs):\n",
    "            \n",
    "            # Print status:\n",
    "            print('    run:',r+1,'of',n_runs)\n",
    "            \n",
    "            # Set up CNN/fc model:\n",
    "            model = set_CNN_fc(\n",
    "                input_length=input_length, \n",
    "                n_features=n_features, \n",
    "                max_pooling_threshold=max_pooling_threshold,\n",
    "                CNN_filters=CNN_filters, \n",
    "                CNN_kernel_sizes=CNN_kernel_sizes, \n",
    "                batch_normalization=batch_normalization,\n",
    "                alpha=alpha,\n",
    "                fc_units=fc_units, \n",
    "                fc_activation=fc_activation,\n",
    "                output_activation=output_activation,\n",
    "                CNN_weight_init=CNN_weight_init,\n",
    "                CNN_bias_init=CNN_bias_init,\n",
    "                fc_weight_init=fc_weight_init,\n",
    "                fc_bias_init=fc_bias_init,\n",
    "                CNN_weight_reg=CNN_weight_reg,\n",
    "                CNN_bias_reg=CNN_bias_reg,\n",
    "                fc_weight_reg=fc_weight_reg,\n",
    "                fc_bias_reg=fc_bias_reg,\n",
    "                learning_rate=learning_rate, \n",
    "                loss_function=loss_function\n",
    "            )\n",
    "            \n",
    "            ### Train model: Epoch-by-epoch\n",
    "            \n",
    "            ## Store results for untrained model:\n",
    "            \n",
    "            # Get model predictions on training, validation and test data:\n",
    "            train_pred = model.predict(train_input)\n",
    "            val_pred = model.predict(val_input)\n",
    "            test_pred = model.predict(test_input)\n",
    "\n",
    "            # Compute mse of model predictions vs. true targets:\n",
    "            train_loss = np.mean((train_target-train_pred)**2)\n",
    "            val_loss = np.mean((val_target-val_pred)**2)\n",
    "            test_loss = np.mean((test_target-test_pred)**2)\n",
    "\n",
    "            # Compute correlation coefficient of model predictions vs. true targets:\n",
    "            train_correl = np.corrcoef(np.stack([train_target[:,0],train_pred[:,0]]))[0,1]\n",
    "            val_correl = np.corrcoef(np.stack([val_target[:,0],val_pred[:,0]]))[0,1]\n",
    "            test_correl = np.corrcoef(np.stack([test_target[:,0],test_pred[:,0]]))[0,1]\n",
    "            \n",
    "            # Store results:\n",
    "            train_loss_all[m,l,r,0] = train_loss\n",
    "            val_loss_all[m,l,r,0] = val_loss\n",
    "            test_loss_all[m,l,r,0] = test_loss\n",
    "            train_correl_all[m,l,r,0] = train_correl\n",
    "            val_correl_all[m,l,r,0] = val_correl\n",
    "            test_correl_all[m,l,r,0] = test_correl          \n",
    "            \n",
    "            # Loop over epochs:\n",
    "            for e in range(n_epochs):\n",
    "                \n",
    "                # Train model for single epoch:\n",
    "                history = model.fit(train_input, train_target, epochs=1, batch_size=batch_size, shuffle=True, verbose=0)\n",
    "\n",
    "                ## Store results after current epoch:\n",
    "            \n",
    "                # Get model predictions on training, validation and test data:\n",
    "                train_pred = model.predict(train_input)\n",
    "                val_pred = model.predict(val_input)\n",
    "                test_pred = model.predict(test_input)\n",
    "\n",
    "                # Compute mse of model predictions vs. true targets:\n",
    "                train_loss = np.mean((train_target-train_pred)**2)\n",
    "                val_loss = np.mean((val_target-val_pred)**2)\n",
    "                test_loss = np.mean((test_target-test_pred)**2)\n",
    "\n",
    "                # Compute correlation coefficient of model predictions vs. true targets:\n",
    "                train_correl = np.corrcoef(np.stack([train_target[:,0],train_pred[:,0]]))[0,1]\n",
    "                val_correl = np.corrcoef(np.stack([val_target[:,0],val_pred[:,0]]))[0,1]\n",
    "                test_correl = np.corrcoef(np.stack([test_target[:,0],test_pred[:,0]]))[0,1]\n",
    "\n",
    "                # Store results:\n",
    "                train_loss_all[m,l,r,e+1] = train_loss\n",
    "                val_loss_all[m,l,r,e+1] = val_loss\n",
    "                test_loss_all[m,l,r,e+1] = test_loss\n",
    "                train_correl_all[m,l,r,e+1] = train_correl\n",
    "                val_correl_all[m,l,r,e+1] = val_correl\n",
    "                test_correl_all[m,l,r,e+1] = test_correl          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ff7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Store results:\n",
    "\n",
    "# ## CNN/fc - univariate:\n",
    "\n",
    "# # Specify model setup:\n",
    "# setup = 'CNN/fc - univariate'\n",
    "\n",
    "# # Save loss and correlation results:\n",
    "# np.save('../results/quickrun_CNN_fc_univariate_train_loss_all.npy', train_loss_all)\n",
    "# np.save('../results/quickrun_CNN_fc_univariate_val_loss_all.npy', val_loss_all)\n",
    "# np.save('../results/quickrun_CNN_fc_univariate_test_loss_all.npy', test_loss_all)\n",
    "# np.save('../results/quickrun_CNN_fc_univariate_train_correl_all.npy', train_correl_all)\n",
    "# np.save('../results/quickrun_CNN_fc_univariate_val_correl_all.npy', val_correl_all)\n",
    "# np.save('../results/quickrun_CNN_fc_univariate_test_correl_all.npy', test_correl_all)\n",
    "\n",
    "# # Store parameters:\n",
    "# parameters = {\n",
    "#     \"setup\": setup,\n",
    "#     \"data_url\": data_url,\n",
    "#     \"target_index\": target_index,\n",
    "#     \"input_features\": input_features,\n",
    "#     \"add_months\": add_months,\n",
    "#     \"norm_target\": norm_target,\n",
    "#     \"input_length\": input_length,\n",
    "#     \"train_test_split\": train_test_split,\n",
    "#     \"train_val_split\": train_val_split,\n",
    "#     \"train_val_split\": train_val_split,\n",
    "#     \"scale_norm\": scale_norm,    \n",
    "#     \"max_pooling_threshold\": max_pooling_threshold,\n",
    "#     \"CNN_filters\": CNN_filters,\n",
    "#     \"CNN_kernel_sizes\": CNN_kernel_sizes,\n",
    "#     \"batch_normalization\": batch_normalization,\n",
    "#     \"alpha\": alpha,\n",
    "#     \"fc_units\": fc_units,\n",
    "#     \"fc_activation\": fc_activation,\n",
    "#     \"output_activation\": output_activation,\n",
    "#     \"CNN_weight_init\": CNN_weight_init,\n",
    "#     \"CNN_bias_init\": CNN_bias_init,\n",
    "#     \"fc_weight_init\": fc_weight_init,\n",
    "#     \"fc_bias_init\": fc_bias_init,\n",
    "#     \"CNN_weight_reg\": CNN_weight_reg,\n",
    "#     \"CNN_bias_reg\": CNN_bias_reg,\n",
    "#     \"fc_weight_reg\": fc_weight_reg,\n",
    "#     \"fc_bias_reg\": fc_bias_reg,\n",
    "#     \"learning_rate\": learning_rate,\n",
    "#     \"loss_function\": loss_function,\n",
    "#     \"ESMs\": ESMs,\n",
    "#     \"lead_times\": lead_times,\n",
    "#     \"n_runs\": n_runs,\n",
    "#     \"n_epochs\": n_epochs,\n",
    "#     \"batch_size\": batch_size,    \n",
    "# }\n",
    "\n",
    "# path_to_store_results = Path('../results')\n",
    "# with open(path_to_store_results / \"quickrun_CNN_fc_univariate_parameters.json\", \"w\") as f:\n",
    "#     dump(parameters, f)\n",
    "\n",
    "# #######################################\n",
    "    \n",
    "# ## CNN/fc - multivariate:\n",
    "\n",
    "# # Specify model setup:\n",
    "# setup = 'CNN/fc - multivariate'\n",
    "\n",
    "# # Save loss and correlation results:\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_train_loss_all.npy', train_loss_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_val_loss_all.npy', val_loss_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_test_loss_all.npy', test_loss_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_train_correl_all.npy', train_correl_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_val_correl_all.npy', val_correl_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_test_correl_all.npy', test_correl_all)\n",
    "\n",
    "# # Store parameters:\n",
    "# parameters = {\n",
    "#     \"setup\": setup,\n",
    "#     \"data_url\": data_url,\n",
    "#     \"target_index\": target_index,\n",
    "#     \"input_features\": input_features,\n",
    "#     \"add_months\": add_months,\n",
    "#     \"norm_target\": norm_target,\n",
    "#     \"input_length\": input_length,\n",
    "#     \"train_test_split\": train_test_split,\n",
    "#     \"train_val_split\": train_val_split,\n",
    "#     \"train_val_split\": train_val_split,\n",
    "#     \"scale_norm\": scale_norm,    \n",
    "#     \"max_pooling_threshold\": max_pooling_threshold,\n",
    "#     \"CNN_filters\": CNN_filters,\n",
    "#     \"CNN_kernel_sizes\": CNN_kernel_sizes,\n",
    "#     \"batch_normalization\": batch_normalization,\n",
    "#     \"alpha\": alpha,\n",
    "#     \"fc_units\": fc_units,\n",
    "#     \"fc_activation\": fc_activation,\n",
    "#     \"output_activation\": output_activation,\n",
    "#     \"CNN_weight_init\": CNN_weight_init,\n",
    "#     \"CNN_bias_init\": CNN_bias_init,\n",
    "#     \"fc_weight_init\": fc_weight_init,\n",
    "#     \"fc_bias_init\": fc_bias_init,\n",
    "#     \"CNN_weight_reg\": CNN_weight_reg,\n",
    "#     \"CNN_bias_reg\": CNN_bias_reg,\n",
    "#     \"fc_weight_reg\": fc_weight_reg,\n",
    "#     \"fc_bias_reg\": fc_bias_reg,\n",
    "#     \"learning_rate\": learning_rate,\n",
    "#     \"loss_function\": loss_function,\n",
    "#     \"ESMs\": ESMs,\n",
    "#     \"lead_times\": lead_times,\n",
    "#     \"n_runs\": n_runs,\n",
    "#     \"n_epochs\": n_epochs,\n",
    "#     \"batch_size\": batch_size,    \n",
    "# }\n",
    "\n",
    "# path_to_store_results = Path('../results')\n",
    "# with open(path_to_store_results / \"quickrun_CNN_fc_multivariate_parameters.json\", \"w\") as f:\n",
    "#     dump(parameters, f)\n",
    "\n",
    "# #######################################\n",
    "    \n",
    "# ## CNN/fc - multivariate - months as additional input features:\n",
    "\n",
    "# # Specify model setup:\n",
    "# setup = 'CNN/fc - multivariate - with months'\n",
    "\n",
    "# # Save loss and correlation results:\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_with_months_train_loss_all.npy', train_loss_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_with_months_val_loss_all.npy', val_loss_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_with_months_test_loss_all.npy', test_loss_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_with_months_train_correl_all.npy', train_correl_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_with_months_val_correl_all.npy', val_correl_all)\n",
    "# np.save('../results/quickrun_CNN_fc_multivariate_with_months_test_correl_all.npy', test_correl_all)\n",
    "\n",
    "# # Store parameters:\n",
    "# parameters = {\n",
    "#     \"setup\": setup,\n",
    "#     \"data_url\": data_url,\n",
    "#     \"target_index\": target_index,\n",
    "#     \"input_features\": input_features,\n",
    "#     \"add_months\": add_months,\n",
    "#     \"norm_target\": norm_target,\n",
    "#     \"input_length\": input_length,\n",
    "#     \"train_test_split\": train_test_split,\n",
    "#     \"train_val_split\": train_val_split,\n",
    "#     \"train_val_split\": train_val_split,\n",
    "#     \"scale_norm\": scale_norm,    \n",
    "#     \"max_pooling_threshold\": max_pooling_threshold,\n",
    "#     \"CNN_filters\": CNN_filters,\n",
    "#     \"CNN_kernel_sizes\": CNN_kernel_sizes,\n",
    "#     \"batch_normalization\": batch_normalization,\n",
    "#     \"alpha\": alpha,\n",
    "#     \"fc_units\": fc_units,\n",
    "#     \"fc_activation\": fc_activation,\n",
    "#     \"output_activation\": output_activation,\n",
    "#     \"CNN_weight_init\": CNN_weight_init,\n",
    "#     \"CNN_bias_init\": CNN_bias_init,\n",
    "#     \"fc_weight_init\": fc_weight_init,\n",
    "#     \"fc_bias_init\": fc_bias_init,\n",
    "#     \"CNN_weight_reg\": CNN_weight_reg,\n",
    "#     \"CNN_bias_reg\": CNN_bias_reg,\n",
    "#     \"fc_weight_reg\": fc_weight_reg,\n",
    "#     \"fc_bias_reg\": fc_bias_reg,\n",
    "#     \"learning_rate\": learning_rate,\n",
    "#     \"loss_function\": loss_function,\n",
    "#     \"ESMs\": ESMs,\n",
    "#     \"lead_times\": lead_times,\n",
    "#     \"n_runs\": n_runs,\n",
    "#     \"n_epochs\": n_epochs,\n",
    "#     \"batch_size\": batch_size,    \n",
    "# }\n",
    "\n",
    "# path_to_store_results = Path('../results')\n",
    "# with open(path_to_store_results / \"quickrun_CNN_fc_multivariate_with_months_parameters.json\", \"w\") as f:\n",
    "#     dump(parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30ff066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Reload results:\n",
    "\n",
    "# ## CNN/fc - univariate:\n",
    "\n",
    "# # Load loss and correlation results:\n",
    "# train_loss_all = np.load('../results/quickrun_CNN_fc_univariate_train_loss_all.npy')\n",
    "# val_loss_all = np.load('../results/quickrun_CNN_fc_univariate_val_loss_all.npy')\n",
    "# test_loss_all = np.load('../results/quickrun_CNN_fc_univariate_test_loss_all.npy',)\n",
    "# train_correl_all = np.load('../results/quickrun_CNN_fc_univariate_train_correl_all.npy')\n",
    "# val_correl_all = np.load('../results/quickrun_CNN_fc_univariate_val_correl_all.npy')\n",
    "# test_correl_all = np.load('../results/quickrun_CNN_fc_univariate_test_correl_all.npy')\n",
    "\n",
    "# # Load parameters:\n",
    "# path_to_store_results = Path('../results')\n",
    "# with open(path_to_store_results / 'quickrun_CNN_fc_univariate_parameters.json', 'r') as f:\n",
    "#     parameters=load(f)\n",
    "\n",
    "# ESMs = parameters['ESMs']\n",
    "# lead_times = parameters['lead_times']\n",
    "# n_runs = parameters['n_runs']\n",
    "# n_epochs = parameters['n_epochs']\n",
    "        \n",
    "\n",
    "# #######################################\n",
    "    \n",
    "# ## CNN/fc - multivariate:\n",
    "\n",
    "# # Load loss and correlation results:\n",
    "# train_loss_all = np.load('../results/quickrun_CNN_fc_multivariate_train_loss_all.npy')\n",
    "# val_loss_all = np.load('../results/quickrun_CNN_fc_multivariate_val_loss_all.npy')\n",
    "# test_loss_all = np.load('../results/quickrun_CNN_fc_multivariate_test_loss_all.npy',)\n",
    "# train_correl_all = np.load('../results/quickrun_CNN_fc_multivariate_train_correl_all.npy')\n",
    "# val_correl_all = np.load('../results/quickrun_CNN_fc_multivariate_val_correl_all.npy')\n",
    "# test_correl_all = np.load('../results/quickrun_CNN_fc_multivariate_test_correl_all.npy')\n",
    "\n",
    "# # Load parameters:\n",
    "# path_to_store_results = Path('../results')\n",
    "# with open(path_to_store_results / 'quickrun_CNN_fc_multivariate_parameters.json', 'r') as f:\n",
    "#     parameters=load(f)\n",
    "\n",
    "# ESMs = parameters['ESMs']\n",
    "# lead_times = parameters['lead_times']\n",
    "# n_runs = parameters['n_runs']\n",
    "# n_epochs = parameters['n_epochs']\n",
    "\n",
    "# #######################################\n",
    "    \n",
    "# ## CNN/fc - multivariate - with months:\n",
    "\n",
    "# # Load loss and correlation results:\n",
    "# train_loss_all = np.load('../results/quickrun_CNN_fc_multivariate_with_months_train_loss_all.npy')\n",
    "# val_loss_all = np.load('../results/quickrun_CNN_fc_multivariate_with_months_val_loss_all.npy')\n",
    "# test_loss_all = np.load('../results/quickrun_CNN_fc_multivariate_with_months_test_loss_all.npy',)\n",
    "# train_correl_all = np.load('../results/quickrun_CNN_fc_multivariate_with_months_train_correl_all.npy')\n",
    "# val_correl_all = np.load('../results/quickrun_CNN_fc_multivariate_with_months_val_correl_all.npy')\n",
    "# test_correl_all = np.load('../results/quickrun_CNN_fc_multivariate_with_months_test_correl_all.npy')\n",
    "\n",
    "# # Load parameters:\n",
    "# path_to_store_results = Path('../results')\n",
    "# with open(path_to_store_results / 'quickrun_CNN_fc_multivariate_with_months_parameters.json', 'r') as f:\n",
    "#     parameters=load(f)\n",
    "\n",
    "# ESMs = parameters['ESMs']\n",
    "# lead_times = parameters['lead_times']\n",
    "# n_runs = parameters['n_runs']\n",
    "# n_epochs = parameters['n_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55aeaa2",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "We now have loss ('mse') and correlation for complete training, validation and test data after each epoch (starting with untrained model as epoch 0).\n",
    "\n",
    "Next, we aim to find the **minimum loss and correlation on test data** for the **epoch with minimum validation loss**.\n",
    "This search is done separately for each ESM, lead time and model run.\n",
    "\n",
    "In a second step, we compute the **mean loss and correlation on test data over all runs**, separately for each ESM and lead time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad6d94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializs storages for test loss (min) and correlation (max), where val loss takes its minimum,\n",
    "## dimension (#ESMs, #lead times, #runs).\n",
    "test_loss_min = np.zeros((len(ESMs),len(lead_times),n_runs))\n",
    "test_correl_max = np.zeros((len(ESMs),len(lead_times),n_runs))\n",
    "\n",
    "## Initializs storages for mean test loss and correlation, averaged over all training runs,\n",
    "## dimension (#ESMs, #lead times).\n",
    "test_loss_min_mean = np.zeros((len(ESMs),len(lead_times)))\n",
    "test_correl_max_mean = np.zeros((len(ESMs),len(lead_times)))\n",
    "\n",
    "## Loop over ESMs:\n",
    "for m in range(len(ESMs)):\n",
    "    \n",
    "    ## Loop over lead times:\n",
    "    for l in range(len(lead_times)):\n",
    "        \n",
    "        # Loop over desired number of training runs:\n",
    "        for r in range(n_runs):\n",
    "            \n",
    "            # Get epoch with minimum validation loss for current ESM, lead time and training run:\n",
    "            e_min = np.argmin(val_loss_all[m,l,r])\n",
    "            \n",
    "            # Store corresponding test loss and correlation:\n",
    "            test_loss_min[m,l,r] = test_loss_all[m,l,r,e_min]\n",
    "            test_correl_max[m,l,r] = test_correl_all[m,l,r,e_min]\n",
    "            \n",
    "        # Get mean test loss and correlation over all training runs, for current ESM and lead time:\n",
    "        test_loss_min_mean[m,l] = np.mean(test_loss_min[m,l])\n",
    "        test_correl_max_mean[m,l] = np.mean(test_correl_max[m,l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009a838",
   "metadata": {},
   "source": [
    "### Results: Univariate CNN/fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e880631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.08345731, 1.09163194, 1.09296049],\n",
       "       [0.8049299 , 0.82506141, 0.82554797]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_min_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20cc09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11017516, 0.07629558, 0.07487747],\n",
       "       [0.16743967, 0.00311626, 0.01481259]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_correl_max_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d545f",
   "metadata": {},
   "source": [
    "### Results: Multivariate CNN/fc (without months as additional input features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b741bb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.02236418, 1.04956418, 1.05474571],\n",
       "       [0.81395483, 0.82134209, 0.83556669]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_min_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de446713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26251132, 0.21423286, 0.20615248],\n",
       "       [0.11630802, 0.06902116, 0.03968716]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_correl_max_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb48db3",
   "metadata": {},
   "source": [
    "### Results: Multivariate CNN/fc (with months as additional input features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52779f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97891029, 0.98520856, 1.04293197],\n",
       "       [0.79110425, 0.82206947, 0.81808267]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_min_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0499eac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33877961, 0.31896233, 0.23212236],\n",
       "       [0.20736868, 0.09712759, 0.08144562]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_correl_max_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d72e81",
   "metadata": {},
   "source": [
    "### Discussion: CNN / fc models - univariate/multivariate (with/without months as additional input features)\n",
    "\n",
    "Here, we tried to **predict future** Sahel rainfall (lead times 1 / 3 / 6 months) from current and past information (t<=0) of all input features (including PREC_SAHEL) with **CNN/fc** models.\n",
    "\n",
    "We started with predicting future Sahel rainfall from its own history alone, hence with **univariate** inputs, which gives us only poor results in terms of high loss ('mse') and low correlation of model predictions and true targets.\n",
    "\n",
    "Adding further input features to have **multivariate** inputs helps to improve prediction accuracy.\n",
    "And ultimately, we added **months as additional input features**, which gives us the best results.\n",
    "\n",
    "However, we find better results for models trained on **CESM** data, compared to **FOCI**.\n",
    "And it appears to be unreasonable, that models trained on univariate inputs outperform models trained on multivariate inputs, at least for FOCI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694dfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
