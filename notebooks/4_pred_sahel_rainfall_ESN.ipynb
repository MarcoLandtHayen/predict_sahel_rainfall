{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7dbe65-6dc0-41a5-aa96-d9a0653e54d6",
   "metadata": {},
   "source": [
    "### Predict Sahel rainfall with Echo State Networks\n",
    "\n",
    "In this project we work with **C**limate **I**ndex **C**ollection based on **Mo**del **D**ata (CICMoD) data set (https://github.com/MarcoLandtHayen/climate_index_collection). \n",
    "\n",
    "Here, we will try to **predict future** Sahel rainfall (lead times 1 / 3 / 6 months) from current and past information (t<=0) of all input features (including PREC_SAHEL) with **ESN** models:\n",
    "\n",
    "- Prepare inputs and targets.\n",
    "- Set up model.\n",
    "- Evaluate model performance.\n",
    "\n",
    "**Note:** We start with predicting future Sahel rainfall from its own history alone, hence with **univariate** inputs. Then, we add further input features to have **multivariate** inputs. And ultimately, we add **months as additional input features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca00440-1fbe-473b-be11-d7cac0887060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from json import dump, load\n",
    "from pathlib import Path\n",
    "\n",
    "### Import additional functions:\n",
    "from predict_sahel_rainfall.plot import bar_color\n",
    "from predict_sahel_rainfall.preprocessing import prepare_inputs_and_target\n",
    "from predict_sahel_rainfall.ESN_functions import ESN, setESN, trainESN, predESN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7b72d",
   "metadata": {},
   "source": [
    "### Prepare inputs and targets: Univariate\n",
    "\n",
    "Load collection of climate indices directly from GitHub release.\n",
    "Use the complete preprocessing pipeline function.\n",
    "**Note:** Don't need validation data, only need training and test data. Hence, set ```train_val_split = 1.0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e280679",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set common parameters (except ESN and lead time) for data preprocessing:\n",
    "\n",
    "# Set url to csv file containing CICMoD indices from desired release:\n",
    "data_url = (\n",
    "    \"https://github.com/MarcoLandtHayen/climate_index_collection/\"\n",
    "    \"releases/download/v2023.03.29.1/climate_indices.csv\"\n",
    ")\n",
    "\n",
    "# Select target index:\n",
    "target_index = 'PREC_SAHEL'\n",
    "\n",
    "# Select all input features:\n",
    "input_features = [\n",
    "    'AMO', 'ENSO_12', 'ENSO_3', 'ENSO_34', 'ENSO_4', 'NAO_PC', 'NAO_ST', \n",
    "    'NP', 'PDO_PC', 'PREC_SAHEL', 'SAM_PC', 'SAM_ZM', 'SAT_N_ALL', 'SAT_N_LAND',\n",
    "    'SAT_N_OCEAN', 'SAT_S_ALL', 'SAT_S_LAND', 'SAT_S_OCEAN', 'SOI',\n",
    "    'SSS_ENA', 'SSS_NA', 'SSS_SA', 'SSS_WNA', 'SST_ESIO', 'SST_HMDR',\n",
    "    'SST_MED', 'SST_TNA', 'SST_TSA', 'SST_WSIO'\n",
    "]\n",
    "\n",
    "# # Select subset of input features:\n",
    "# input_features = [\n",
    "#     'PREC_SAHEL',\n",
    "# ]\n",
    "\n",
    "# Choose, whether to add months as one-hot encoded features:\n",
    "add_months = True\n",
    "\n",
    "# Choose, whether to normalize target index:\n",
    "norm_target = True\n",
    "\n",
    "# Specify input length:\n",
    "input_length = 24\n",
    "\n",
    "# Specify amount of combined training and validation data relative to test data:\n",
    "train_test_split = 0.9\n",
    "\n",
    "# Specify relative amount of combined training and validation used for training:\n",
    "train_val_split = 1.0\n",
    "\n",
    "## Optionally choose to scale or normalize input features according to statistics from training data:\n",
    "# 'no': Keep raw input features.\n",
    "# 'scale_01': Scale input features with min/max scaling to [0,1].\n",
    "# 'scale_11': Scale input features with min/max scaling to [-1,1].\n",
    "# 'norm': Normalize input features, hence subtract mean and divide by std dev.\n",
    "scale_norm = 'scale_11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a39753e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for ESN model:\n",
    "verbose = False\n",
    "n_layers = 1 # Number of ESN layers in the model.\n",
    "n_res = 300 # Number of reservoir units.\n",
    "W_in_lim = 0.1 # Initialize input weights from random uniform distribution in [- W_in_lim, + W_in_lim]\n",
    "leak_rate = 0.05 # Leak rate used in transition function of reservoir states.\n",
    "leak_rate_first_step_YN = True # If true, multiply with alpha already in calculating first timestes's res. states.\n",
    "leaky_integration_YN = True # If True, multiply previous time steps' reservoir states with (1-a).\n",
    "                            # If False, omit multiplication with (1-a) in reservoir state transition. \n",
    "                            # But in any case still multiply new time steps' input (and reservoir recurrence) \n",
    "                            # with leakrate a after activation.\n",
    "activation = 'tanh' # Desired activation function to be used in calculating reservoir state transition.\n",
    "spec_radius = 0.8 # Spectral radius, becomes largest Eigenvalue of reservoir weight matrix.\n",
    "sparsity = 0.3 # Sparsity of reservoir weight matrix.\n",
    "out_features = 1 # Single target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec18f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features: 41\n"
     ]
    }
   ],
   "source": [
    "# Set choice of ESMs:\n",
    "ESMs = ['CESM', 'FOCI']\n",
    "\n",
    "# Set choice of lead times:\n",
    "lead_times = [1,3,6]\n",
    "\n",
    "# Set number of runs per setting:\n",
    "n_runs = 3\n",
    "\n",
    "# Get number of input features, depending on whether or not months are added as additional features:\n",
    "if add_months:\n",
    "    n_features = len(input_features) + 12\n",
    "else:\n",
    "    n_features = len(input_features)\n",
    "\n",
    "# Check number of input channels and input length:\n",
    "print('Number of input features:',n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c666ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "071b98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for test:\n",
    "input_length = 24\n",
    "n_features = 10\n",
    "verbose = False\n",
    "n_layers = 1\n",
    "n_res = 300\n",
    "W_in_lim = 0.1\n",
    "leak_rate = 0.05\n",
    "leak_rate_first_step_YN = True\n",
    "leaky_integration_YN = True\n",
    "activation = 'tanh'\n",
    "spec_radius = 0.8\n",
    "sparsity = 0.3\n",
    "out_features = 1\n",
    "\n",
    "\n",
    "model, model_short, all_states = setESN(\n",
    "    input_length=input_length, \n",
    "    in_features=n_features,                                        \n",
    "    out_features=out_features, \n",
    "    n_layers=n_layers,                                        \n",
    "    n_res=n_res, \n",
    "    W_in_lim=W_in_lim, \n",
    "    leak_rate=leak_rate,\n",
    "    leak_rate_first_step_YN=leak_rate_first_step_YN,\n",
    "    leaky_integration_YN = leaky_integration_YN,\n",
    "    activation=activation, \n",
    "    spec_radius=spec_radius,\n",
    "    sparsity=sparsity, \n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9718a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd9b00f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM: 1 of 2\n",
      "  lead time: 1 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "  lead time: 2 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "  lead time: 3 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "ESM: 2 of 2\n",
      "  lead time: 1 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "  lead time: 2 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n",
      "  lead time: 3 of 3\n",
      "    run: 1 of 3\n",
      "    run: 2 of 3\n",
      "    run: 3 of 3\n"
     ]
    }
   ],
   "source": [
    "## Initializs storages for loss curves and correlation, dimension (#ESMs, #lead times, #runs).\n",
    "train_loss_all = np.zeros((len(ESMs),len(lead_times),n_runs))\n",
    "test_loss_all = np.zeros((len(ESMs),len(lead_times),n_runs))\n",
    "train_correl_all = np.zeros((len(ESMs),len(lead_times),n_runs))\n",
    "test_correl_all = np.zeros((len(ESMs),len(lead_times),n_runs))\n",
    "\n",
    "## Loop over ESMs:\n",
    "for m in range(len(ESMs)):\n",
    "    \n",
    "    # Get current ESM:\n",
    "    ESM = ESMs[m]\n",
    "    \n",
    "    # Print status:\n",
    "    print('ESM:',m+1,'of',len(ESMs))\n",
    "\n",
    "    ## Loop over lead times:\n",
    "    for l in range(len(lead_times)):\n",
    "        \n",
    "        # Get current lead time:\n",
    "        lead_time = lead_times[l]\n",
    "        \n",
    "        # Print status:\n",
    "        print('  lead time:',l+1,'of',len(lead_times))\n",
    "\n",
    "        # Prepare inputs and target for current ESM and lead time:\n",
    "        (\n",
    "            train_input,\n",
    "            train_target,\n",
    "            _,\n",
    "            _,\n",
    "            test_input,\n",
    "            test_target,\n",
    "            train_mean,\n",
    "            train_std,\n",
    "            train_min,\n",
    "            train_max,\n",
    "        ) = prepare_inputs_and_target(    \n",
    "            data_url=data_url,\n",
    "            ESM=ESM,\n",
    "            target_index=target_index,\n",
    "            input_features=input_features,\n",
    "            add_months=add_months,\n",
    "            norm_target=norm_target,\n",
    "            lead_time=lead_time,\n",
    "            input_length=input_length,\n",
    "            train_test_split=train_test_split,\n",
    "            train_val_split=train_val_split,\n",
    "            scale_norm=scale_norm,\n",
    "        )\n",
    "        \n",
    "        # Add dummy column of ONEs as first input time step:\n",
    "        train_input = np.concatenate([np.ones((train_input.shape[0],1,train_input.shape[2])), train_input], axis=1)\n",
    "        test_input = np.concatenate([np.ones((test_input.shape[0],1,test_input.shape[2])), test_input], axis=1)\n",
    "\n",
    "        # Loop over desired number of training runs:\n",
    "        for r in range(n_runs):\n",
    "            \n",
    "            # Print status:\n",
    "            print('    run:',r+1,'of',n_runs)\n",
    "            \n",
    "            ## Set up ESN model:\n",
    "            # Get complete model (output = target prediction) plus short model (output final reservoir states from all layers)\n",
    "            # and all_states (= another shortened model that gives reservoir states for ALL time steps for all inputs).\n",
    "            # Manually add dummy column to input length:\n",
    "            model, model_short, all_states = setESN(\n",
    "                input_length=input_length+1, \n",
    "                in_features=n_features,                                        \n",
    "                out_features=out_features, \n",
    "                n_layers=n_layers,                                        \n",
    "                n_res=n_res, \n",
    "                W_in_lim=W_in_lim, \n",
    "                leak_rate=leak_rate,\n",
    "                leak_rate_first_step_YN=leak_rate_first_step_YN,\n",
    "                leaky_integration_YN = leaky_integration_YN,\n",
    "                activation=activation, \n",
    "                spec_radius=spec_radius,\n",
    "                sparsity=sparsity, \n",
    "                verbose=verbose)\n",
    "            \n",
    "            # Train ESN model's output weights and bias\n",
    "            model = trainESN(model, model_short, train_input, train_target, verbose=verbose)\n",
    "\n",
    "            # Get predictions from trained ESN model and evaluation metrics on model performance:\n",
    "            (\n",
    "                train_pred, \n",
    "                test_pred, \n",
    "                _, \n",
    "                _, \n",
    "                _, \n",
    "                _\n",
    "            ) = predESN(model, train_input, test_input, train_target, test_target, verbose=verbose)\n",
    "            \n",
    "            # Compute mse of model predictions vs. true targets:\n",
    "            train_loss = np.mean((train_target-train_pred)**2)\n",
    "            test_loss = np.mean((test_target-test_pred)**2)\n",
    "\n",
    "            # Compute correlation coefficient of model predictions vs. true targets:\n",
    "            train_correl = np.corrcoef(np.stack([train_target[:,0],train_pred[:,0]]))[0,1]\n",
    "            test_correl = np.corrcoef(np.stack([test_target[:,0],test_pred[:,0]]))[0,1]\n",
    "            \n",
    "            # Store results:\n",
    "            train_loss_all[m,l,r] = train_loss\n",
    "            test_loss_all[m,l,r] = test_loss\n",
    "            train_correl_all[m,l,r] = train_correl\n",
    "            test_correl_all[m,l,r] = test_correl       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62250cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39ff7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store results:\n",
    "\n",
    "# ## ESN - multivariate:\n",
    "\n",
    "# # Specify model setup:\n",
    "# setup = 'ESN - multivariate'\n",
    "\n",
    "# # Save loss and correlation results:\n",
    "# np.save('../results/quickrun_ESN_multivariate_train_loss_all.npy', train_loss_all)\n",
    "# np.save('../results/quickrun_ESN_multivariate_test_loss_all.npy', test_loss_all)\n",
    "# np.save('../results/quickrun_ESN_multivariate_train_correl_all.npy', train_correl_all)\n",
    "# np.save('../results/quickrun_ESN_multivariate_test_correl_all.npy', test_correl_all)\n",
    "\n",
    "# # Store parameters:\n",
    "# parameters = {\n",
    "#     \"setup\": setup,\n",
    "#     \"data_url\": data_url,\n",
    "#     \"target_index\": target_index,\n",
    "#     \"input_features\": input_features,\n",
    "#     \"add_months\": add_months,\n",
    "#     \"norm_target\": norm_target,\n",
    "#     \"input_length\": input_length,\n",
    "#     \"train_test_split\": train_test_split,\n",
    "#     \"train_val_split\": train_val_split,\n",
    "#     \"train_val_split\": train_val_split,\n",
    "#     \"scale_norm\": scale_norm,  \n",
    "#     \"verbose\": verbose,  \n",
    "#     \"n_layers\": n_layers,  \n",
    "#     \"n_res\": n_res,  \n",
    "#     \"W_in_lim\": W_in_lim,  \n",
    "#     \"leak_rate\": leak_rate,  \n",
    "#     \"leak_rate_first_step_YN\": leak_rate_first_step_YN,  \n",
    "#     \"leaky_integration_YN\": leaky_integration_YN,  \n",
    "#     \"activation\": activation,  \n",
    "#     \"spec_radius\": spec_radius,  \n",
    "#     \"sparsity\": sparsity,  \n",
    "#     \"out_features\": out_features,  \n",
    "#     \"ESMs\": ESMs,\n",
    "#     \"lead_times\": lead_times,\n",
    "#     \"n_runs\": n_runs,   \n",
    "# }\n",
    "\n",
    "# path_to_store_results = Path('../results')\n",
    "# with open(path_to_store_results / \"quickrun_ESN_multivariate_parameters.json\", \"w\") as f:\n",
    "#     dump(parameters, f)\n",
    "\n",
    "# #######################################\n",
    "    \n",
    "## ESN - multivariate - months as additional input features:\n",
    "\n",
    "# Specify model setup:\n",
    "setup = 'ESN - multivariate - with months'\n",
    "\n",
    "# Save loss and correlation results:\n",
    "np.save('../results/quickrun_ESN_multivariate_with_months_train_loss_all.npy', train_loss_all)\n",
    "np.save('../results/quickrun_ESN_multivariate_with_months_test_loss_all.npy', test_loss_all)\n",
    "np.save('../results/quickrun_ESN_multivariate_with_months_train_correl_all.npy', train_correl_all)\n",
    "np.save('../results/quickrun_ESN_multivariate_with_months_test_correl_all.npy', test_correl_all)\n",
    "\n",
    "# Store parameters:\n",
    "parameters = {\n",
    "    \"setup\": setup,\n",
    "    \"data_url\": data_url,\n",
    "    \"target_index\": target_index,\n",
    "    \"input_features\": input_features,\n",
    "    \"add_months\": add_months,\n",
    "    \"norm_target\": norm_target,\n",
    "    \"input_length\": input_length,\n",
    "    \"train_test_split\": train_test_split,\n",
    "    \"train_val_split\": train_val_split,\n",
    "    \"train_val_split\": train_val_split,\n",
    "    \"scale_norm\": scale_norm,  \n",
    "    \"verbose\": verbose,  \n",
    "    \"n_layers\": n_layers,  \n",
    "    \"n_res\": n_res,  \n",
    "    \"W_in_lim\": W_in_lim,  \n",
    "    \"leak_rate\": leak_rate,  \n",
    "    \"leak_rate_first_step_YN\": leak_rate_first_step_YN,  \n",
    "    \"leaky_integration_YN\": leaky_integration_YN,  \n",
    "    \"activation\": activation,  \n",
    "    \"spec_radius\": spec_radius,  \n",
    "    \"sparsity\": sparsity,  \n",
    "    \"out_features\": out_features,  \n",
    "    \"ESMs\": ESMs,\n",
    "    \"lead_times\": lead_times,\n",
    "    \"n_runs\": n_runs,   \n",
    "}\n",
    "\n",
    "path_to_store_results = Path('../results')\n",
    "with open(path_to_store_results / \"quickrun_ESN_multivariate_with_months_parameters.json\", \"w\") as f:\n",
    "    dump(parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30ff066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Reload results:\n",
    "   \n",
    "# ## ESN - multivariate:\n",
    "\n",
    "# # Load loss and correlation results:\n",
    "# train_loss_all = np.load('../results/quickrun_ESN_multivariate_train_loss_all.npy')\n",
    "# test_loss_all = np.load('../results/quickrun_ESN_multivariate_test_loss_all.npy',)\n",
    "# train_correl_all = np.load('../results/quickrun_ESN_multivariate_train_correl_all.npy')\n",
    "# test_correl_all = np.load('../results/quickrun_ESN_multivariate_test_correl_all.npy')\n",
    "\n",
    "# # Load parameters:\n",
    "# path_to_store_results = Path('../results')\n",
    "# with open(path_to_store_results / 'quickrun_ESN_multivariate_parameters.json', 'r') as f:\n",
    "#     parameters=load(f)\n",
    "\n",
    "# ESMs = parameters['ESMs']\n",
    "# lead_times = parameters['lead_times']\n",
    "# n_runs = parameters['n_runs']\n",
    "\n",
    "# #######################################\n",
    "    \n",
    "# ## ESN - multivariate - with months:\n",
    "\n",
    "# # Load loss and correlation results:\n",
    "# train_loss_all = np.load('../results/quickrun_ESN_multivariate_with_months_train_loss_all.npy')\n",
    "# test_loss_all = np.load('../results/quickrun_ESN_multivariate_with_months_test_loss_all.npy',)\n",
    "# train_correl_all = np.load('../results/quickrun_ESN_multivariate_with_months_train_correl_all.npy')\n",
    "# test_correl_all = np.load('../results/quickrun_ESN_multivariate_with_months_test_correl_all.npy')\n",
    "\n",
    "# # Load parameters:\n",
    "# path_to_store_results = Path('../results')\n",
    "# with open(path_to_store_results / 'quickrun_ESN_multivariate_with_months_parameters.json', 'r') as f:\n",
    "#     parameters=load(f)\n",
    "\n",
    "# ESMs = parameters['ESMs']\n",
    "# lead_times = parameters['lead_times']\n",
    "# n_runs = parameters['n_runs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55aeaa2",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "We now have loss ('mse') and correlation for complete training and test data.\n",
    "\n",
    "Next, we compute the **mean loss and correlation on test data over all runs**, separately for each ESM and lead time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad6d94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializs storages for mean test loss and correlation, averaged over all training runs,\n",
    "## dimension (#ESMs, #lead times).\n",
    "test_loss_mean = np.zeros((len(ESMs),len(lead_times)))\n",
    "test_correl_mean = np.zeros((len(ESMs),len(lead_times)))\n",
    "\n",
    "## Loop over ESMs:\n",
    "for m in range(len(ESMs)):\n",
    "    \n",
    "    ## Loop over lead times:\n",
    "    for l in range(len(lead_times)):        \n",
    "            \n",
    "        # Get mean test loss and correlation over all training runs, for current ESM and lead time:\n",
    "        test_loss_mean[m,l] = np.mean(test_loss_all[m,l])\n",
    "        test_correl_mean[m,l] = np.mean(test_correl_all[m,l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d545f",
   "metadata": {},
   "source": [
    "### Results: Multivariate ESN (without months as additional input features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22fe668f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.08662774, 1.07734265, 1.09522613],\n",
       "       [0.84196932, 0.8732742 , 0.88330637]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dabc1126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20153117, 0.20912285, 0.16223111],\n",
       "       [0.15897595, 0.08291692, 0.0533908 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_correl_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb48db3",
   "metadata": {},
   "source": [
    "### Results: Multivariate ESN (with months as additional input features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52779f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.04180678, 1.06451105, 1.11768725],\n",
       "       [0.83749761, 0.85866585, 0.85737492]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0499eac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2557714 , 0.22284854, 0.15976369],\n",
       "       [0.1840896 , 0.10410937, 0.07351633]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_correl_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d72e81",
   "metadata": {},
   "source": [
    "### Discussion: ESN models - multivariate (with/without months as additional input features)\n",
    "\n",
    "Here, we tried to **predict future** Sahel rainfall (lead times 1 / 3 / 6 months) from current and past information (t<=0) of all input features (including PREC_SAHEL) with **ESN** models.\n",
    "\n",
    "We skipped predicting future Sahel rainfall from its own history alone, hence with **univariate** inputs.\n",
    "Instead, we directly started with adding further input features to have **multivariate** inputs.\n",
    "And then, we added **months as additional input features**, which gives us the slightly improved results.\n",
    "\n",
    "However, we find better results for models trained on **CESM** data, compared to **FOCI**.\n",
    "And we see unreasonable behaviour, e.g., for models trained on CESM data with lead time 6: Lowest loss is found for models trained on multivariate inputs and slightly higher loss for models trained on multivariate inputs including months. Would have expected the reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694dfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
